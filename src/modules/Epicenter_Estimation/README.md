# Epicenter Estimation

This project aims to estimate the epicenter of seismic events using advanced algorithms and data analysis techniques. 

## Table of Contents
- [Introduction](#introduction)
- [Installation](#installation)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)

## Introduction
In the field of seismology, accurately determining the epicenter of an earthquake is crucial for understanding its impact and potential risks. This project focuses on developing a robust and efficient method for estimating the epicenter using seismic data.

## Installation
To use this project, follow these steps:

1. Clone the repository: `git clone https://github.com/lptvUchile/Epicenter_Estimation`
2. Install the required dependencies using [pdm](https://pdm-project.org/en/latest/): `pdm install`
3. Download lightweight assets using [dvc](https://dvc.org/): `dvc pull`
4. Download waves assets using (requires zip & unzip to be installed): `sh download_assets.sh`


### DVC Setup

1. Download your JSON file associated to the Google Service Account.
2. Move your file to `~/.gdrive/gdrive.json`.
3. In the repository, run:
```sh
dvc remote modify mainremote gdrive_use_service_account true
vc remote modify mainremote --local \
              gdrive_service_account_json_file_path ~/.gdrive/gdrive.json
```
`4. Run `dvc pull` to get latest changes of the data, results and models.
`
## Usage

### Data folder
After using DVC for pulling the data, you will see many folders.
- coastline: has the shape files with the coastline of Chile, it is used for post-processing the epicenter model evaluations to measure the localization of coast earthquakes.
- features: is generated by every training with `--extract_features` argument.
- inventory: has xml inventories of every supported station of networks C, CX and C1.
- sacs: has the raw event file for vel & acc instruments.
- set: has every split dataset for val, test and train for every type.
- tests: has csv files for every training with some metrics for distance and back-azimuth models for every seed used.
- every .dvc file is a reference handled by dvc for storing in cloud the data.

### Running tests

Remember to save your data in your local repository via dvc by using:
```sh
dvc add results models data/tests
dvc push
git add results.dvc models.dvc data/tests.dvc
git commit -m "Update results, models and tests"
git push
```
Then, run `dvc pull -f` when checkout to this commit, you will lose your local data changes.

To run a test, you can use the following command for distance model:
```sh
pdm train-distance --test_file=./data/tests/dist_1.yaml --extract_features
```
And for back-azimuth model:
```sh
pdm train-backazimuth --test_file=./data/tests/ba_1.yaml --extract_features
```
If you want to extract features, you can use the `--extract_features` argument.

If you want to test the code or functionality of the codebase, you can use --test argument to make the extraction of features for a small subset of the data.

This is the default configuration file for back-azimuth:
```yaml
feature: mayores4M
binary: true
window_limit: [1, 3]
seeds: [3322, 1122, 1234, 4321, 1111, 4354, 8712, 9210]
accel: true
global_feat: true
response: "vel"
patience: 30
lr: 0.00015
epochs: 1000
filter:
  - bandpass
  - [0.5, 10]
dummy: false
```

This is the default configuration file for distance:
```yaml
feature: mayores4M
binary: true
window_limit: [5, 120]
seeds: [3322, 1122, 1234, 4321, 1111, 4354, 8712, 9210]
accel: true
global_feat: true
response: "vel"
patience: 25
lr: 0.0001
epochs: 100
umbral_corte: 0
filter:
  - highpass
  - 1
dummy: false
```

If dummy is true, the model will be trained with the epochs count and restoring the best weights based on validation data, if false, the model will be trained with the patience count based on validation data with an early stopping restoring the best weights.

For distance, if umbral_corte is 0, the model will segment the event data based on the window_limit, if not, the model will segment the event data based on percentage of the energy of the event, for example, if umbral_corte is 0.03, the model will segment the event data based on the end to 3% of the energy of the event.

Every training will be saved in the results folder with the name of the test file and the seed used.
The command for running the training will train for every seed in the test file and will save the results in the results folder.

The `train.sh` script file will run the training for every test file in the tests folder. If the test has "ba" in the name, it will run the back-azimuth model, if "dist" in the name, it will run the distance model.

Metrics are saved in a `results` folder in the same directory of the test file, so, if using:
```sh
pdm train-distance --test_file=./data/tests/dist_1.yaml --extract_features
```
The metrics will be saved in `./data/tests/results/dist_1.csv`.